#!/usr/bin/env python3
"""
douban_top250_crawler.py
æŠ“å–è±†ç“£ Top250 ç”µå½±ä¿¡æ¯
"""

import sys
import time
import re
import requests
import yaml
from bs4 import BeautifulSoup
from urllib.parse import quote_plus

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/123.0 Safari/537.36"
    )
}

def crawl_douban_top250():
    """æŠ“å–è±†ç“£Top250ç”µå½±ä¿¡æ¯ï¼Œè¿”å› [(title_cn, title_en, year, rating, director)] åˆ—è¡¨"""
    movies = []
    
    print("ğŸ“¥ å¼€å§‹æŠ“å–è±†ç“£ Top250 â€¦")
    
    for start in range(0, 250, 25):  # 10 é¡µ Ã— 25 æ¡
        url = f"https://movie.douban.com/top250?start={start}"
        print(f"æ­£åœ¨æŠ“å–ç¬¬ {start//25 + 1} é¡µ: {url}")
        
        try:
            response = requests.get(url, headers=HEADERS, timeout=20)
            response.raise_for_status()
            html = response.text
            soup = BeautifulSoup(html, "html.parser")

            for item in soup.select("div.item"):
                try:
                    # æå–æ ‡é¢˜ä¿¡æ¯
                    hd_div = item.select_one("div.hd")
                    if not hd_div:
                        continue
                        
                    titles = list(hd_div.stripped_strings)
                    title_cn = titles[0].strip()
                    title_en = titles[1].strip() if len(titles) > 1 else ""
                    
                    # æå–å¹´ä»½
                    bd_div = item.select_one("div.bd p")
                    if not bd_div:
                        continue
                        
                    bd_text = bd_div.get_text()
                    # ä½¿ç”¨æœ€ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…4ä½æ•°å­—
                    year_match = re.search(r"(\d{4})", bd_text)
                    year = year_match.group(1) if year_match else ""
                    

                    
                    # æå–è¯„åˆ†
                    rating_span = item.select_one("span.rating_num")
                    rating = rating_span.get_text().strip() if rating_span else ""
                    
                    # æå–å¯¼æ¼”ä¿¡æ¯
                    director_match = re.search(r"å¯¼æ¼”:\s*([^ä¸»]+)", bd_text)
                    director = director_match.group(1).strip() if director_match else ""
                    
                    # æå–ä¸»æ¼”ä¿¡æ¯
                    actor_match = re.search(r"ä¸»æ¼”:\s*([^\.]+)", bd_text)
                    actors = actor_match.group(1).strip() if actor_match else ""
                    
                    # æå–æ’å
                    rank_span = item.select_one("em")
                    rank = rank_span.get_text().strip() if rank_span else ""
                    
                    print(f"è±†ç“£æŠ“å–: [{rank}] {title_cn} / {title_en} ({year}) - è¯„åˆ†:{rating} - å¯¼æ¼”:{director}")
                    
                    movies.append({
                        'rank': rank,
                        'title_cn': title_cn,
                        'title_en': title_en,
                        'year': year,
                        'rating': rating,
                        'director': director,
                        'actors': actors
                    })
                    
                except Exception as e:
                    print(f"è§£æç”µå½±æ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue
                    
            time.sleep(1)  # ç¤¼è²Œå»¶æ—¶ï¼Œé¿å…è¢«è±†ç“£å°
            
        except requests.RequestException as e:
            print(f"è¯·æ±‚ç¬¬ {start//25 + 1} é¡µæ—¶å‡ºé”™: {e}")
            continue
        except Exception as e:
            print(f"å¤„ç†ç¬¬ {start//25 + 1} é¡µæ—¶å‡ºé”™: {e}")
            continue
    
    print(f"âœ… æŠ“å–å®Œæˆï¼Œå…±è·å– {len(movies)} éƒ¨ç”µå½±")
    return movies

def save_to_file(movies, filename="douban_top250.yml"):
    """å°†ç”µå½±ä¿¡æ¯ä¿å­˜åˆ°YAMLæ–‡ä»¶"""
    # æ„å»ºYAMLæ•°æ®ç»“æ„
    yaml_data = {
        "è±†ç“£Top250ç”µå½±åˆ—è¡¨": {
            "summary": "è±†ç“£è¯„åˆ†æœ€é«˜çš„250éƒ¨ç”µå½±åˆé›†",
            "total_count": len(movies),
            "movies": []
        }
    }
    
    for movie in movies:
        # è°ƒè¯•ä¿¡æ¯
        print(f"ä¿å­˜ç”µå½±: æ’å={movie['rank']}, å¹´ä»½='{movie['year']}'")
        
        movie_data = {
            "rank": int(movie['rank']),
            "title_cn": movie['title_cn'],
            "title_en": movie['title_en'],
            "year": int(movie['year']) if movie['year'] else None,
            "rating": float(movie['rating']) if movie['rating'] else None,
            "director": movie['director'],
            "actors": movie['actors']
        }
        yaml_data["è±†ç“£Top250ç”µå½±åˆ—è¡¨"]["movies"].append(movie_data)
    
    # ä¿å­˜ä¸ºYAMLæ–‡ä»¶
    with open(filename, "w", encoding="utf-8") as f:
        yaml.dump(yaml_data, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ° {filename}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ è±†ç“£Top250ç”µå½±æŠ“å–å·¥å…·")
    print("=" * 30)
    
    try:
        # æŠ“å–ç”µå½±ä¿¡æ¯
        movies = crawl_douban_top250()
        
        if movies:
            # ä¿å­˜åˆ°æ–‡ä»¶
            save_to_file(movies)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"æ€»ç”µå½±æ•°: {len(movies)}")
            
            # æŒ‰å¹´ä»½ç»Ÿè®¡
            year_count = {}
            for movie in movies:
                year = movie['year']
                year_count[year] = year_count.get(year, 0) + 1
            
            print(f"å¹´ä»½åˆ†å¸ƒ:")
            for year in sorted(year_count.keys(), reverse=True):
                print(f"  {year}: {year_count[year]} éƒ¨")
                
        else:
            print("âŒ æœªæŠ“å–åˆ°ä»»ä½•ç”µå½±ä¿¡æ¯")
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main() 